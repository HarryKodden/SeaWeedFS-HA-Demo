{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b48490e",
   "metadata": {},
   "source": [
    "# SeaweedFS Active-Active Resilience Demonstration\n",
    "\n",
    "This notebook demonstrates the fault-tolerance and resilience capabilities of SeaweedFS in a high-availability setup. We'll use the existing `docker-compose-ha.yml` configuration to:\n",
    "\n",
    "1. Start a full HA cluster with 3 master nodes, 3 volume servers, and 2 filer servers\n",
    "2. Upload test data to the system\n",
    "3. Simulate various failure scenarios (master node failure, volume server failure)\n",
    "4. Monitor the system's recovery and healing process\n",
    "5. Verify data integrity after recovery\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The high-availability setup consists of:\n",
    "\n",
    "- **3 Master Servers**: Running as a Raft cluster for consensus and leader election\n",
    "- **3 Volume Servers**: Each on a different logical rack for data distribution\n",
    "- **2 Filer Servers**: Providing redundant access to the file system \n",
    "- **NGINX Load Balancer**: Distributing requests between filer instances\n",
    "\n",
    "```\n",
    "                   ┌──────────────┐\n",
    "                   │     NGINX    │\n",
    "                   │  Load Balancer│\n",
    "                   └───────┬──────┘\n",
    "                           │\n",
    "            ┌──────────────┴───────────────┐\n",
    "            │                              │\n",
    "     ┌──────┴────────┐            ┌────────┴──────┐\n",
    "     │    Filer1     │            │     Filer2    │\n",
    "     └──────┬────────┘            └────────┬──────┘\n",
    "            │                              │\n",
    "            └──────────────┬───────────────┘\n",
    "                           │\n",
    "         ┌────────────┬────┴────┬────────────┐\n",
    "         │            │         │            │\n",
    "  ┌──────┴─────┐ ┌────┴────┐ ┌──┴───────┐ ┌──┴────────┐\n",
    "  │  Master1   │ │ Master2 │ │ Master3  │ │          │\n",
    "  │  (Leader)  │ │         │ │          │ │          │\n",
    "  └──────┬─────┘ └────┬────┘ └──┬───────┘ │  Volume  │\n",
    "         │            │         │         │  Servers │\n",
    "         └────────────┴────┬────┘         │          │\n",
    "                           │              │          │\n",
    "                      ┌────┴────┐         │          │\n",
    "                      │ Volume1 │         │          │\n",
    "                      │ Volume2 ├─────────┘          │\n",
    "                      │ Volume3 │                    │\n",
    "                      └─────────┘                    │\n",
    "```\n",
    "\n",
    "## SeaweedFS Resilience Features\n",
    "\n",
    "1. **Master Server Resilience**: Multiple masters in a Raft cluster ensure leadership failover if the leader goes down\n",
    "2. **Data Redundancy**: Volume data can be replicated across multiple volume servers\n",
    "3. **Rack Awareness**: Data distribution across different racks to survive rack failures\n",
    "4. **Automatic Rebalancing**: When a volume server is added back, data is rebalanced\n",
    "5. **Filer Redundancy**: Multiple filer servers provide uninterrupted access to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import importlib.util\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a Python package using the recommended python -m pip approach\"\"\"\n",
    "    if importlib.util.find_spec(package.split('==')[0]) is None:\n",
    "        print(f\"Installing {package}...\")\n",
    "        try:\n",
    "            # Use subprocess to run python -m pip install\n",
    "            result = subprocess.run(\n",
    "                [sys.executable, '-m', 'pip', 'install', package],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            print(f\"Successfully installed {package}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "            print(f\"Error output: {e.stderr}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"{package.split('==')[0]} is already installed.\")\n",
    "        return True\n",
    "\n",
    "# List of required packages\n",
    "packages = ['requests', 'pandas', 'numpy', 'matplotlib', 'seaborn', 'python-dotenv', 'boto3', 'plotly', 'docker']\n",
    "\n",
    "# Install each package\n",
    "for package in packages:\n",
    "    success = install_package(package)\n",
    "    if not success:\n",
    "        print(f\"Warning: Failed to install {package}. Some notebook features may not work correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import docker\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import datetime\n",
    "import random\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import re\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path('/home/harry/projects/seaweedfs/demo/.env')\n",
    "    if env_path.exists():\n",
    "        print(f\"Loading environment variables from: {env_path}\")\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "    else:\n",
    "        print(\"No .env file found. Using default environment variables.\")\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed. Using default environment variables.\")\n",
    "\n",
    "# Setup Docker client\n",
    "client = docker.from_env()\n",
    "\n",
    "# Configuration for SeaweedFS API endpoints - we'll use local ports from docker-compose-ha.yml\n",
    "master1_url = \"http://localhost:9333\"  # Master1 port\n",
    "master2_url = \"http://localhost:9334\"  # Master2 port\n",
    "master3_url = \"http://localhost:9335\"  # Master3 port\n",
    "volume1_url = \"http://localhost:8080\"  # Volume1 port\n",
    "volume2_url = \"http://localhost:8081\"  # Volume2 port \n",
    "volume3_url = \"http://localhost:8082\"  # Volume3 port\n",
    "filer_url = \"http://localhost:9000\"    # NGINX load balancer port\n",
    "s3_url = \"http://localhost:9000/s3\"    # S3 API through NGINX\n",
    "\n",
    "# Auth details\n",
    "auth_user = os.getenv(\"SEAWEED_AUTH_USER\", None)\n",
    "auth_password = os.getenv(\"SEAWEED_AUTH_PASSWORD\", None) \n",
    "auth = None if not auth_user else (auth_user, auth_password)\n",
    "\n",
    "# S3 credentials\n",
    "s3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"seaweedfs\")  # Default from docker-compose-ha.yml\n",
    "s3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"seaweedfs\")  # Default from docker-compose-ha.yml\n",
    "\n",
    "print(f\"Master1 URL: {master1_url}\")\n",
    "print(f\"Master2 URL: {master2_url}\")\n",
    "print(f\"Master3 URL: {master3_url}\")\n",
    "print(f\"Volume1 URL: {volume1_url}\")\n",
    "print(f\"Volume2 URL: {volume2_url}\")\n",
    "print(f\"Volume3 URL: {volume3_url}\")\n",
    "print(f\"Filer URL (NGINX): {filer_url}\")\n",
    "print(f\"S3 URL: {s3_url}\")\n",
    "print(f\"Authentication: {'Enabled' if auth else 'Disabled'}\")\n",
    "print(f\"S3 Access Key: {s3_access_key}\")\n",
    "print(f\"S3 Secret Key: {'*' * len(s3_secret_key)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078db2cd",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define some helper functions to interact with the SeaweedFS cluster, monitor its state, and perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9298dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(method, url, params=None, data=None, headers=None, files=None, auth=auth, timeout=5):\n",
    "    \"\"\"Make an API request to the SeaweedFS servers with timeout and error handling\"\"\"\n",
    "    try:\n",
    "        response = requests.request(\n",
    "            method=method,\n",
    "            url=url,\n",
    "            params=params,\n",
    "            data=data,\n",
    "            headers=headers,\n",
    "            files=files,\n",
    "            auth=auth,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Try to parse as JSON\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making API request to {url}: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response status code: {e.response.status_code}\")\n",
    "            try:\n",
    "                print(f\"Response text: {e.response.text[:200]}...\")\n",
    "            except:\n",
    "                pass\n",
    "        return None\n",
    "\n",
    "def format_table(data, title=None):\n",
    "    \"\"\"Format data as a styled HTML table\"\"\"\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        if not data:\n",
    "            return HTML(\"<p>No data available</p>\")\n",
    "        df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        df = pd.DataFrame([data])\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    try:\n",
    "        # Try to use pandas styling\n",
    "        styled = df.style.set_table_attributes('style=\"border-collapse:collapse\"')\n",
    "        styled = styled.set_properties(**{'border': '1px solid black', 'padding': '5px'})\n",
    "        styled = styled.set_table_styles([\n",
    "            {'selector': 'th', 'props': [('background-color', '#f2f2f2'), \n",
    "                                       ('border', '1px solid black'),\n",
    "                                       ('padding', '5px'),\n",
    "                                       ('text-align', 'left')]}\n",
    "        ])\n",
    "        \n",
    "        if title:\n",
    "            return HTML(f\"<h3>{title}</h3>\" + styled.to_html())\n",
    "        return HTML(styled.to_html())\n",
    "    except Exception as e:\n",
    "        # Fallback if styling fails\n",
    "        print(f\"Warning: Using unstyled table because: {str(e)}\")\n",
    "        html = df.to_html(border=1, index=False)\n",
    "        if title:\n",
    "            html = f\"<h3>{title}</h3>{html}\"\n",
    "        return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker container management functions\n",
    "\n",
    "def get_container_by_service(service_name):\n",
    "    \"\"\"Get container by service name (e.g. 'master1', 'volume2')\"\"\"\n",
    "    try:\n",
    "        containers = client.containers.list()\n",
    "        for container in containers:\n",
    "            if service_name in container.name:\n",
    "                return container\n",
    "        print(f\"Container for service '{service_name}' not found\")\n",
    "        return None\n",
    "    except docker.errors.APIError as e:\n",
    "        print(f\"Docker API error: {e}\")\n",
    "        return None\n",
    "\n",
    "def stop_container(service_name):\n",
    "    \"\"\"Stop a container by service name\"\"\"\n",
    "    container = get_container_by_service(service_name)\n",
    "    if container:\n",
    "        print(f\"Stopping container {container.name}...\")\n",
    "        container.stop()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def start_container(service_name):\n",
    "    \"\"\"Start a container by service name\"\"\"\n",
    "    container = get_container_by_service(service_name)\n",
    "    if container:\n",
    "        print(f\"Starting container {container.name}...\")\n",
    "        container.start()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_container_status(service_name):\n",
    "    \"\"\"Get status of a container by service name\"\"\"\n",
    "    container = get_container_by_service(service_name)\n",
    "    if container:\n",
    "        container.reload()  # Refresh container info\n",
    "        return container.status\n",
    "    return \"not found\"\n",
    "\n",
    "def list_containers():\n",
    "    \"\"\"List all containers related to the SeaweedFS cluster\"\"\"\n",
    "    try:\n",
    "        containers = client.containers.list(all=True)\n",
    "        seaweed_containers = []\n",
    "        seaweed_services = ['master', 'volume', 'filer', 'nginx']\n",
    "        \n",
    "        for container in containers:\n",
    "            if any(service in container.name for service in seaweed_services):\n",
    "                seaweed_containers.append({\n",
    "                    'id': container.short_id,\n",
    "                    'name': container.name,\n",
    "                    'status': container.status,\n",
    "                    'image': container.image.tags[0] if container.image.tags else container.image.short_id\n",
    "                })\n",
    "        \n",
    "        return seaweed_containers\n",
    "    except docker.errors.APIError as e:\n",
    "        print(f\"Docker API error: {e}\")\n",
    "        return []\n",
    "\n",
    "def display_container_status():\n",
    "    \"\"\"Display current status of all SeaweedFS containers\"\"\"\n",
    "    containers = list_containers()\n",
    "    if containers:\n",
    "        return format_table(containers, \"SeaweedFS Container Status\")\n",
    "    else:\n",
    "        return HTML(\"<p>No SeaweedFS containers found</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeaweedFS system status monitoring functions\n",
    "\n",
    "def get_cluster_status(master_url=master1_url):\n",
    "    \"\"\"Get the status of the SeaweedFS cluster\"\"\"\n",
    "    return api_request('GET', f\"{master_url}/cluster/status?pretty=y\")\n",
    "\n",
    "def get_system_topology(master_url=master1_url):\n",
    "    \"\"\"Get the system topology information\"\"\"\n",
    "    return api_request('GET', f\"{master_url}/dir/status?pretty=y\")\n",
    "\n",
    "def get_volume_status(master_url=master1_url):\n",
    "    \"\"\"Get the status of all volumes in the system\"\"\"\n",
    "    return api_request('GET', f\"{master_url}/vol/status?pretty=y\")\n",
    "\n",
    "def check_master_leader():\n",
    "    \"\"\"Determine which master is currently the leader\"\"\"\n",
    "    masters = [master1_url, master2_url, master3_url]\n",
    "    for i, master_url in enumerate(masters):\n",
    "        try:\n",
    "            status = api_request('GET', f\"{master_url}/cluster/status\")\n",
    "            if status and 'Leader' in status:\n",
    "                leader_url = status['Leader']\n",
    "                leader_name = f\"master{i+1}\"\n",
    "                print(f\"✅ Leader master is {leader_name} at {leader_url}\")\n",
    "                return leader_name, leader_url\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"❌ No leader master found\")\n",
    "    return None, None\n",
    "\n",
    "def check_endpoint_health():\n",
    "    \"\"\"Check health of all endpoints\"\"\"\n",
    "    endpoints = {\n",
    "        \"master1\": f\"{master1_url}/cluster/status\",\n",
    "        \"master2\": f\"{master2_url}/cluster/status\",\n",
    "        \"master3\": f\"{master3_url}/cluster/status\",\n",
    "        \"volume1\": f\"{volume1_url}/status\",\n",
    "        \"volume2\": f\"{volume2_url}/status\",\n",
    "        \"volume3\": f\"{volume3_url}/status\",\n",
    "        \"filer\": f\"{filer_url}/\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, url in endpoints.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.get(url, timeout=2)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"service\": name,\n",
    "                \"status\": \"✅ Online\" if response.status_code < 400 else \"⚠️ Error\",\n",
    "                \"code\": response.status_code,\n",
    "                \"response_time\": f\"{elapsed:.3f}s\"\n",
    "            })\n",
    "        except requests.exceptions.RequestException:\n",
    "            results.append({\n",
    "                \"service\": name,\n",
    "                \"status\": \"❌ Offline\",\n",
    "                \"code\": \"N/A\",\n",
    "                \"response_time\": \"N/A\"\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def monitor_system_health(interval=5, duration=60):\n",
    "    \"\"\"Monitor system health for a specified duration\"\"\"\n",
    "    end_time = time.time() + duration\n",
    "    health_data = []\n",
    "    \n",
    "    try:\n",
    "        while time.time() < end_time:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "            leader, _ = check_master_leader()\n",
    "            health_results = check_endpoint_health()\n",
    "            \n",
    "            # Record data\n",
    "            record = {\"timestamp\": timestamp, \"leader\": leader}\n",
    "            for result in health_results:\n",
    "                record[result[\"service\"]] = \"Online\" if \"✅\" in result[\"status\"] else \"Offline\"\n",
    "            \n",
    "            health_data.append(record)\n",
    "            \n",
    "            # Display current status\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<h2>System Health Monitoring</h2><p>Current time: {timestamp}</p>\"))\n",
    "            display(format_table(health_results, \"Current Endpoint Status\"))\n",
    "            \n",
    "            if len(health_data) > 1:\n",
    "                # Create and display chart\n",
    "                plot_health_data(health_data)\n",
    "            \n",
    "            # Wait for next interval\n",
    "            remaining = min(interval, end_time - time.time())\n",
    "            if remaining > 0:\n",
    "                time.sleep(remaining)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped by user\")\n",
    "    \n",
    "    return health_data\n",
    "\n",
    "def plot_health_data(health_data):\n",
    "    \"\"\"Plot health data over time\"\"\"\n",
    "    df = pd.DataFrame(health_data)\n",
    "    \n",
    "    # Get service columns (exclude timestamp and leader)\n",
    "    service_cols = [col for col in df.columns if col not in [\"timestamp\", \"leader\"]]\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    for col in service_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == \"Online\" else 0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot service status\n",
    "    for col in service_cols:\n",
    "        plt.plot(df[\"timestamp\"], df[col], marker=\"o\", label=col)\n",
    "    \n",
    "    # Highlight leader changes\n",
    "    leader_changes = df[df[\"leader\"] != df[\"leader\"].shift(1)].index\n",
    "    for idx in leader_changes:\n",
    "        if idx > 0:  # Skip the first point\n",
    "            plt.axvline(x=idx, color=\"r\", linestyle=\"--\", alpha=0.3)\n",
    "            plt.text(idx, 0.5, f\"Leader: {df.iloc[idx]['leader']}\", \n",
    "                    rotation=90, verticalalignment=\"center\")\n",
    "    \n",
    "    plt.yticks([0, 1], [\"Offline\", \"Online\"])\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"SeaweedFS System Health Over Time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Status\")\n",
    "    plt.legend(loc=\"lower left\", bbox_to_anchor=(0, 1.02, 1, 0.2), mode=\"expand\", ncol=4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 API interaction functions\n",
    "\n",
    "def create_s3_client():\n",
    "    \"\"\"Create an S3 client to interact with the SeaweedFS S3 API\"\"\"\n",
    "    endpoint_url = s3_url\n",
    "    if not endpoint_url.startswith(\"http\"):\n",
    "        endpoint_url = f\"http://{endpoint_url}\"\n",
    "        \n",
    "    try:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=s3_access_key,\n",
    "            aws_secret_access_key=s3_secret_key,\n",
    "            # For self-signed certificates\n",
    "            verify=False,\n",
    "            # Required for non-AWS S3 implementations\n",
    "            config=boto3.session.Config(\n",
    "                signature_version='s3v4',\n",
    "                s3={'addressing_style': 'path'}\n",
    "            )\n",
    "        )\n",
    "        return s3_client\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating S3 client: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_test_bucket(bucket_name=\"resilience-test\"):\n",
    "    \"\"\"Create a test bucket for experiments\"\"\"\n",
    "    s3_client = create_s3_client()\n",
    "    if not s3_client:\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        # Check if bucket exists first\n",
    "        try:\n",
    "            s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' already exists\")\n",
    "            return True\n",
    "        except:\n",
    "            # Bucket doesn't exist, create it\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "            print(f\"Created bucket '{bucket_name}'\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_test_data(size_kb=100):\n",
    "    \"\"\"Generate random test data of specified size\"\"\"\n",
    "    return os.urandom(size_kb * 1024)\n",
    "\n",
    "def upload_test_files(bucket_name=\"resilience-test\", file_count=10, size_kb=100):\n",
    "    \"\"\"Upload test files to the bucket\"\"\"\n",
    "    s3_client = create_s3_client()\n",
    "    if not s3_client:\n",
    "        return []\n",
    "        \n",
    "    uploaded_files = []\n",
    "    \n",
    "    try:\n",
    "        # Create the bucket if it doesn't exist\n",
    "        create_test_bucket(bucket_name)\n",
    "        \n",
    "        # Upload files\n",
    "        for i in range(file_count):\n",
    "            key = f\"test-file-{i}.dat\"\n",
    "            data = generate_test_data(size_kb)\n",
    "            \n",
    "            s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=key,\n",
    "                Body=io.BytesIO(data)\n",
    "            )\n",
    "            \n",
    "            uploaded_files.append({\n",
    "                \"bucket\": bucket_name,\n",
    "                \"key\": key,\n",
    "                \"size\": size_kb,\n",
    "                \"md5\": hash(data) # Simple hash for verification\n",
    "            })\n",
    "            \n",
    "            print(f\"Uploaded {key} ({size_kb}KB)\")\n",
    "            \n",
    "        return uploaded_files\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading test files: {e}\")\n",
    "        return uploaded_files\n",
    "\n",
    "def verify_files(file_list):\n",
    "    \"\"\"Verify that files are accessible and match expected content\"\"\"\n",
    "    s3_client = create_s3_client()\n",
    "    if not s3_client:\n",
    "        return []\n",
    "        \n",
    "    results = []\n",
    "    \n",
    "    for file_info in file_list:\n",
    "        try:\n",
    "            response = s3_client.get_object(\n",
    "                Bucket=file_info[\"bucket\"],\n",
    "                Key=file_info[\"key\"]\n",
    "            )\n",
    "            \n",
    "            data = response[\"Body\"].read()\n",
    "            current_hash = hash(data)\n",
    "            is_match = current_hash == file_info[\"md5\"]\n",
    "            \n",
    "            results.append({\n",
    "                \"key\": file_info[\"key\"],\n",
    "                \"accessible\": True,\n",
    "                \"size_match\": len(data) == file_info[\"size\"]*1024,\n",
    "                \"content_match\": is_match,\n",
    "                \"status\": \"✅ Valid\" if is_match else \"❌ Corrupted\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"key\": file_info[\"key\"],\n",
    "                \"accessible\": False,\n",
    "                \"size_match\": False,\n",
    "                \"content_match\": False,\n",
    "                \"status\": \"❌ Not accessible\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c19ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "\n",
    "def plot_topology():\n",
    "    \"\"\"Visualize the SeaweedFS cluster topology\"\"\"\n",
    "    try:\n",
    "        # Get topology information\n",
    "        topology = get_system_topology()\n",
    "        if not topology:\n",
    "            print(\"Could not fetch topology information\")\n",
    "            return\n",
    "        \n",
    "        # Extract data centers, racks, and data nodes\n",
    "        datacenters = topology.get(\"Topology\", {}).get(\"DataCenters\", [])\n",
    "        \n",
    "        # Create network graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add master node\n",
    "        G.add_node(\"Master\", role=\"master\", size=20)\n",
    "        \n",
    "        # Add data centers, racks, and volumes\n",
    "        for dc_idx, dc in enumerate(datacenters):\n",
    "            dc_name = dc.get(\"Id\", f\"DC-{dc_idx}\")\n",
    "            G.add_node(dc_name, role=\"datacenter\", size=15)\n",
    "            G.add_edge(\"Master\", dc_name)\n",
    "            \n",
    "            for rack_idx, rack in enumerate(dc.get(\"Racks\", [])):\n",
    "                rack_name = rack.get(\"Id\", f\"{dc_name}-Rack-{rack_idx}\")\n",
    "                G.add_node(rack_name, role=\"rack\", size=10)\n",
    "                G.add_edge(dc_name, rack_name)\n",
    "                \n",
    "                for server_idx, server in enumerate(rack.get(\"DataNodes\", [])):\n",
    "                    server_name = f\"Volume-{server_idx}-{rack_name}\"\n",
    "                    G.add_node(server_name, role=\"volume\", size=5, volumes=server.get(\"Volumes\", []))\n",
    "                    G.add_edge(rack_name, server_name)\n",
    "        \n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "        \n",
    "        # Colormap based on role\n",
    "        color_map = {\n",
    "            'master': 'red',\n",
    "            'datacenter': 'blue',\n",
    "            'rack': 'green',\n",
    "            'volume': 'orange'\n",
    "        }\n",
    "        node_colors = [color_map[G.nodes[node]['role']] for node in G.nodes()]\n",
    "        node_sizes = [G.nodes[node]['size'] * 100 for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)\n",
    "        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "        \n",
    "        plt.title(\"SeaweedFS Cluster Topology\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing topology: {e}\")\n",
    "    \n",
    "def animate_failure_recovery(duration=30, interval=0.5):\n",
    "    \"\"\"Create an animation showing system state during failure and recovery\"\"\"\n",
    "    # This requires stored monitoring data from monitor_system_health\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d6d31",
   "metadata": {},
   "source": [
    "## 1. Check If Cluster Is Running\n",
    "\n",
    "First, let's check if our SeaweedFS high-availability cluster is running by inspecting the Docker containers and API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if our cluster is already running\n",
    "display_container_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c79976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the cluster if it's not already running\n",
    "def ensure_cluster_running():\n",
    "    \"\"\"Ensure the SeaweedFS HA cluster is running\"\"\"\n",
    "    try:\n",
    "        # Check if Docker Compose exists\n",
    "        docker_compose_path = '/home/harry/projects/seaweedfs/demo/docker-compose-ha.yml'\n",
    "        if not os.path.exists(docker_compose_path):\n",
    "            print(f\"Error: docker-compose-ha.yml not found at {docker_compose_path}\")\n",
    "            return False\n",
    "            \n",
    "        # Check if containers are already running\n",
    "        containers = list_containers()\n",
    "        running_containers = [c for c in containers if c['status'] == 'running']\n",
    "        \n",
    "        if running_containers and len(running_containers) >= 9:  # 3 masters, 3 volumes, 2 filers, 1 nginx\n",
    "            print(\"SeaweedFS HA cluster is already running\")\n",
    "            return True\n",
    "            \n",
    "        # Start the cluster using docker-compose\n",
    "        print(\"Starting SeaweedFS HA cluster...\")\n",
    "        current_dir = os.getcwd()\n",
    "        os.chdir(os.path.dirname(docker_compose_path))\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            ['docker-compose', '-f', 'docker-compose-ha.yml', 'up', '-d'], \n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        os.chdir(current_dir)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"SeaweedFS HA cluster started successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error starting cluster: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error ensuring cluster is running: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the function\n",
    "ensure_cluster_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce1d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which master is the leader\n",
    "check_master_leader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check health of all endpoints\n",
    "health_results = check_endpoint_health()\n",
    "format_table(health_results, \"Initial System Health Check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609513a",
   "metadata": {},
   "source": [
    "## 2. Examine System Topology\n",
    "\n",
    "Let's examine the system topology to understand the structure of our SeaweedFS HA cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dcde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display cluster status\n",
    "cluster_status = get_cluster_status()\n",
    "print(json.dumps(cluster_status, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253357a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display system topology\n",
    "topology = get_system_topology()\n",
    "print(json.dumps(topology, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display volume status\n",
    "volume_status = get_volume_status()\n",
    "print(json.dumps(volume_status, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b532d5",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Data\n",
    "\n",
    "To demonstrate resilience, we'll create some test data that we can use to verify system integrity after failure scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test bucket\n",
    "create_test_bucket(\"resilience-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test files for resilience testing\n",
    "uploaded_files = upload_test_files(bucket_name=\"resilience-test\", file_count=20, size_kb=500)\n",
    "format_table(uploaded_files, \"Uploaded Test Files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d184da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files are accessible\n",
    "verification_results = verify_files(uploaded_files)\n",
    "format_table(verification_results, \"Initial File Verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f0acc",
   "metadata": {},
   "source": [
    "## 4. Master Node Failure Scenario\n",
    "\n",
    "In this scenario, we'll simulate a master node failure and observe how the system handles it.\n",
    "\n",
    "### Step 1: Identify Current Leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the current leader\n",
    "leader_name, leader_url = check_master_leader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start monitoring system health\n",
    "print(\"Starting health monitoring. Will fail the leader master in 10 seconds...\")\n",
    "print(\"Press Ctrl+C to stop monitoring\")\n",
    "\n",
    "# Start monitoring in a separate thread\n",
    "monitor_thread = threading.Thread(target=lambda: monitor_system_health(interval=2, duration=120))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait 10 seconds before failing the leader\n",
    "time.sleep(10)\n",
    "\n",
    "# Fail the leader master\n",
    "if leader_name:\n",
    "    print(f\"⚠️ Stopping {leader_name} (current leader)...\")\n",
    "    stop_container(leader_name)\n",
    "else:\n",
    "    print(\"No leader found, cannot perform failure test\")\n",
    "\n",
    "# Wait for monitoring to complete\n",
    "monitor_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check who the new leader is\n",
    "new_leader_name, new_leader_url = check_master_leader()\n",
    "\n",
    "if new_leader_name and new_leader_name != leader_name:\n",
    "    print(f\"✅ Leader election successful! New leader: {new_leader_name}\")\n",
    "elif new_leader_name == leader_name:\n",
    "    print(f\"❌ Leader did not change: still {leader_name}\")\n",
    "else:\n",
    "    print(\"❌ No leader found after failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64216783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that files are still accessible after master failure\n",
    "print(\"Verifying file accessibility after master failure...\")\n",
    "verification_results = verify_files(uploaded_files)\n",
    "format_table(verification_results, \"File Verification After Master Failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b78006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the failed master\n",
    "if leader_name:\n",
    "    print(f\"Restarting {leader_name}...\")\n",
    "    start_container(leader_name)\n",
    "    print(f\"Waiting for {leader_name} to initialize...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Check container status\n",
    "    status = get_container_status(leader_name)\n",
    "    print(f\"{leader_name} status: {status}\")\n",
    "else:\n",
    "    print(\"No leader name recorded, cannot restart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f75338",
   "metadata": {},
   "source": [
    "## 5. Volume Server Failure Scenario\n",
    "\n",
    "In this scenario, we'll simulate a volume server failure and observe data redundancy and healing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start monitoring system health\n",
    "print(\"Starting health monitoring. Will fail volume1 in 10 seconds...\")\n",
    "print(\"Press Ctrl+C to stop monitoring\")\n",
    "\n",
    "# Start monitoring in a separate thread\n",
    "monitor_thread = threading.Thread(target=lambda: monitor_system_health(interval=2, duration=120))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait 10 seconds before failing the volume\n",
    "time.sleep(10)\n",
    "\n",
    "# Fail volume1\n",
    "print(\"⚠️ Stopping volume1...\")\n",
    "stop_container('volume1')\n",
    "\n",
    "# Wait for monitoring to complete\n",
    "monitor_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system topology after volume1 failure\n",
    "print(\"Checking system topology after volume1 failure...\")\n",
    "topology = get_system_topology()\n",
    "print(json.dumps(topology, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eeeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that files are still accessible after volume server failure\n",
    "print(\"Verifying file accessibility after volume server failure...\")\n",
    "verification_results = verify_files(uploaded_files)\n",
    "format_table(verification_results, \"File Verification After Volume Server Failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b192ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the failed volume\n",
    "print(\"Restarting volume1...\")\n",
    "start_container('volume1')\n",
    "print(\"Waiting for volume1 to initialize...\")\n",
    "time.sleep(5)\n",
    "    \n",
    "# Check container status\n",
    "status = get_container_status('volume1')\n",
    "print(f\"volume1 status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097d3c5",
   "metadata": {},
   "source": [
    "## 6. Multiple Server Failure Scenario\n",
    "\n",
    "In this scenario, we'll simulate multiple simultaneous failures to test the system's limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ae8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start monitoring system health\n",
    "print(\"Starting health monitoring. Will fail multiple servers in 10 seconds...\")\n",
    "print(\"Press Ctrl+C to stop monitoring\")\n",
    "\n",
    "# Start monitoring in a separate thread\n",
    "monitor_thread = threading.Thread(target=lambda: monitor_system_health(interval=2, duration=180))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n",
    "\n",
    "# Wait 10 seconds before failing servers\n",
    "time.sleep(10)\n",
    "\n",
    "# Fail multiple servers\n",
    "print(\"⚠️ Stopping master2, volume2, and filer2...\")\n",
    "stop_container('master2')\n",
    "stop_container('volume2')\n",
    "stop_container('filer2')\n",
    "\n",
    "# Wait for monitoring to complete\n",
    "monitor_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a94752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that files are still accessible after multiple server failures\n",
    "print(\"Verifying file accessibility after multiple server failures...\")\n",
    "verification_results = verify_files(uploaded_files)\n",
    "format_table(verification_results, \"File Verification After Multiple Server Failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the failed servers\n",
    "print(\"Restarting master2, volume2, and filer2...\")\n",
    "start_container('master2')\n",
    "start_container('volume2')\n",
    "start_container('filer2')\n",
    "print(\"Waiting for servers to initialize...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Check container status\n",
    "containers = list_containers()\n",
    "format_table(containers, \"Container Status After Restart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47af14",
   "metadata": {},
   "source": [
    "## 7. Simulating System Healing\n",
    "\n",
    "Let's observe how the system heals after failures by monitoring topology changes and data rebalancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get volume status after all failures and recoveries\n",
    "volume_status = get_volume_status()\n",
    "print(json.dumps(volume_status, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check endpoint health after all recoveries\n",
    "health_results = check_endpoint_health()\n",
    "format_table(health_results, \"Final System Health Check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad011e",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "Based on our experiments, we can draw the following conclusions about SeaweedFS resilience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b61533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final file verification after all tests\n",
    "final_verification = verify_files(uploaded_files)\n",
    "format_table(final_verification, \"Final File Verification After All Tests\")\n",
    "\n",
    "# Summary statistics\n",
    "total_files = len(final_verification)\n",
    "accessible_files = sum(1 for file in final_verification if file['accessible'])\n",
    "content_match = sum(1 for file in final_verification if file['content_match'])\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total files: {total_files}\")\n",
    "print(f\"- Files still accessible: {accessible_files} ({accessible_files/total_files*100:.1f}%)\")\n",
    "print(f\"- Files with intact content: {content_match} ({content_match/total_files*100:.1f}%)\")\n",
    "\n",
    "if accessible_files == total_files and content_match == total_files:\n",
    "    print(\"\\n✅ SUCCESS: All files remained accessible and intact through the failure scenarios.\")\n",
    "    print(\"This demonstrates SeaweedFS's excellent resilience against server failures.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ WARNING: {total_files - accessible_files} files were not accessible or had content issues.\")\n",
    "    print(\"This may indicate configuration issues with replication or healing processes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda6e2a",
   "metadata": {},
   "source": [
    "### SeaweedFS Resilience Features Demonstrated\n",
    "\n",
    "1. **Leader Election**: When a master leader fails, another master automatically takes over leadership\n",
    "\n",
    "2. **Data Redundancy**: With proper replication settings, data remains accessible even when a volume server fails\n",
    "\n",
    "3. **Service Continuity**: The system continues to operate despite server failures\n",
    "\n",
    "4. **Self-Healing**: When servers are restored, they rejoin the cluster and synchronize data\n",
    "\n",
    "5. **Load Balancing**: NGINX provides continuous access to filer services even when one filer is down\n",
    "\n",
    "### Recommendations for Production Systems\n",
    "\n",
    "1. **Replication Strategy**: Use at least `001` (one copy in each rack) for critical data\n",
    "\n",
    "2. **Physical Separation**: Deploy masters and volume servers on physically separate hardware\n",
    "\n",
    "3. **Monitoring**: Implement continuous monitoring of all SeaweedFS components\n",
    "\n",
    "4. **Backup Strategy**: Even with high availability, maintain regular backups\n",
    "\n",
    "5. **Volume Distribution**: Ensure volumes are well-distributed across available storage nodes\n",
    "\n",
    "6. **Network Redundancy**: Implement redundant network paths between components\n",
    "\n",
    "7. **Regular Testing**: Periodically test failure scenarios to ensure resilience mechanisms work as expected"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
